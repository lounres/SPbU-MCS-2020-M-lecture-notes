\documentclass[12pt,a4paper]{article}
\usepackage{solutions}
\usepackage{float}
\usepackage{multicol}
\usepackage{inkscape}
\usepackage{dsfont}
\usepackage[all]{xy}
\CompileMatrices

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\title{Листочек 1.\\Дискретная теория вероятностей. 1 курс.\\Решения.}
\author{Глеб Минаев @ 102 (20.Б02-мкн)}
% \date{}

\DeclareMathOperator{\sign}{sign}
\newcommand{\DD}{\ensuremath{\mathbb{D}}\xspace}
\renewcommand{\Re}{\qopname\relax o{Re}}
\renewcommand{\Im}{\qopname\relax o{Im}}

\begin{document}
    \maketitle

    \begin{multicols}{2}
        \tableofcontents
    \end{multicols}

    \begin{enumproblem}
        Предположим можно. Пусть $p_i$ и $q_i$ ($i \in \{1; \dots; n\}$) суть вероятности выпадения числа $i$ на первом и втором кубиках соответственно.
        
        Мы хотим, чтобы сумма выпавших чисел была равномерно распределена на множестве $\{2; \dots; 12\}$, а следовательно, чтобы вероятность выпадения каждой из описанных сумм равнялась $1/11$. В частности
        \begin{align*}
            &p_1 \cdot q_1 = \frac{1}{11}&
            &p_6 \cdot q_6 = \frac{1}{11}&
            &p_1 \cdot q_6 + p_2 \cdot q_5 + \cdots + p_6 \cdot q_1 = \frac{1}{11}&
        \end{align*}
        Тогда по неравенству между средним арифметическим и средним геометрическим мы имеем, что
        \[
            \frac{1}{11}
            \geqslant p_1 \cdot q_6 + p_6 \cdot q_1
            \geqslant 2\sqrt{(p_1 \cdot q_6)(p_6 \cdot q_1)}
            = 2\sqrt{(p_1 \cdot q_1)(p_6 \cdot q_6)}
            = \frac{2}{11}
        \]
        --- противоречие. Значит достичь данной цели невозможно.
    \end{enumproblem}

    \begin{enumproblem}
        Обозначим всякую конфигурацию из $X$ спичек, взятых из первого коробка, и $Y$ спичек, взятых из второго коробка, за $(X; Y)$. Мы начинаем в $(0; 0)$ и путешествуем по прямоугольнику $(n+1) \times (M + 1)$. Каждым шагом мы можем увеличить ровно одну координату на единицу, если это возможно, причём, какую конкретно операцию выполнять, мы выбираем случайно (с равномерно распределённой вероятностью). И мы ищем мат. ожидание числа $Y$ такого, что $(n; Y)$ --- первая посещённая конфигурация с первой координатой равной $n$.

        Пусть первой такой точкой мы посетили $(n; T)$, что $T < M$. Тогда в неё мы пришли из $(n-1; T)$, а значит у нас было $\binom{T+n-1}{n-1}$ способов добраться в неё, и в каждый момент у нас был выбор из двух направлений (так как никакая координата не упиралась в свой максимум; только в самом конце). Следовательно вероятность такого события равна
        \[\frac{\binom{T+n-1}{n-1}}{2^{T+n}}\]
        Теперь посчитаем эту же вероятность, но для $T = M$. Заметим, что в этом случае мы сначала попали в какую-то точку $(k; M-1)$ ($k < n$), следующим шагом попали в $(k; M)$, а затем и в $(n; M)$. Таким образом все пути разбиваются на классы по значениям $k$. Фиксируем какое-нибудь $k$. Тогда способов пройти таким образом ровно $\binom{k+M-1}{k}$; при этом выбор, куда пойти, у нас был вплоть до вершины $(k; M)$. Следовательно вероятность каждого такого пути равна $1/2^{M+k}$. Следовательно суммарная вероятность (т.е. вероятность того, что $Y = M$) равна
        \[\sum_{k=0}^{n-1} \frac{\binom{k+M-1}{k}}{2^{M+k}}\]

        Таким образом
        \begin{gather*}
            \EE(Y) = \sum_{T=0}^{M-1} T \cdot \frac{\binom{T+n-1}{n-1}}{2^{T+n}} + M \cdot \sum_{k=0}^{n-1} \frac{\binom{k+M-1}{k}}{2^{M+k}}\\
            \EE(Y^2) = \sum_{T=0}^{M-1} T^2 \cdot \frac{\binom{T+n-1}{n-1}}{2^{T+n}} + M^2 \cdot \sum_{k=0}^{n-1} \frac{\binom{k+M-1}{k}}{2^{M+k}}
        \end{gather*}

        Заметим, что для $T > 0$
        \[T \cdot \frac{\binom{T+n-1}{n-1}}{2^{T+n}} = n \cdot \frac{\binom{T+n-1}{n}}{2^{T+n}}\]
        и следовательно
        \[
            \sum_{T=0}^{M-1} T \cdot \frac{\binom{T+n-1}{n-1}}{2^{T+n}}
            = \sum_{T=1}^{M-1} n \cdot \frac{\binom{T+n-1}{n}}{2^{T+n}}
            = n\sum_{T=0}^{(M-1)-1} \frac{\binom{T+(n+1)-1}{(n+1)-1}}{2^{T+(n+1)}}
            = n\left(1 - \sum_{k=0}^{n} \frac{\binom{k+(M-1)-1}{k}}{2^{(M-1)+k}}\right)
        \]
        Поясним последний переход. Заметим, что вероятность добраться до какой-либо точки $(n; T)$ рано или поздно равна $1$, а следовательно
        \[1 = \sum_{T=0}^{M-1} \frac{\binom{T+n-1}{n-1}}{2^{T+n}} + \sum_{k=0}^{n-1} \frac{\binom{k+M-1}{k}}{2^{M+k}}\]
        (говоря по-другому, мы написали, что $\EE(Y^0) = 1$). В последнем же переходе мы применили это равенство для $n+1$ и $M-1$ вместо $n$ и $M$ соответственно.

        Аналогично сделаем тот же трюк для $T(T-1)$.
        \[
            T(T-1) \cdot \frac{\binom{T+n-1}{n-1}}{2^{T+n}}
            = n(T-1) \cdot \frac{\binom{T+n-1}{n}}{2^{T+n}}
            = n(n+1) \cdot \frac{\binom{T+n-1}{n+1}}{2^{T+n}}
        \]
        и следовательно
        \begin{multline*}
            \sum_{T=0}^{M-1} T(T-1) \cdot \frac{\binom{T+n-1}{n-1}}{2^{T+n}}
            = \sum_{T=2}^{M-1} n(n+1) \cdot \frac{\binom{T+n-1}{n+1}}{2^{T+n}}\\
            = n(n+1)\sum_{T=0}^{(M-2)-1} \frac{\binom{T+(n+2)-1}{(n+2)-1}}{2^{T+(n+2)}}
            = n(n+1)\left(1 - \sum_{k=0}^{n+1} \frac{\binom{k+(M-2)-1}{k}}{2^{(M-2)+k}}\right)
        \end{multline*}

        Таким образом мы получаем, что
        \begin{gather*}
            \EE(Y) = n\left(1 - \sum_{k=0}^{n} \frac{\binom{k+(M-1)-1}{k}}{2^{(M-1)+k}}\right) + M \cdot \sum_{k=0}^{n-1} \frac{\binom{k+M-1}{k}}{2^{M+k}}\\
            \EE(Y^2) = n\left(1 - \sum_{k=0}^{n} \frac{\binom{k+(M-1)-1}{k}}{2^{(M-1)+k}}\right) + n(n+1)\left(1 - \sum_{k=0}^{n+1} \frac{\binom{k+(M-2)-1}{k}}{2^{(M-2)+k}}\right) + M^2 \cdot \sum_{k=0}^{n-1} \frac{\binom{k+M-1}{k}}{2^{M+k}}
        \end{gather*}
        Дисперсия легко получается по соотношению $\DD(Y) = \EE(Y^2) - \EE(Y)^2$, но получится что-то ещё более страшное и менее сокращаемое.

        Теперь определим асимптотику по $M$.
        \begin{align*}
            \EE(Y) - n
            &= -n \cdot \sum_{k=0}^{n} \frac{\binom{k+(M-1)-1}{k}}{2^{(M-1)+k}} + M \cdot \sum_{k=0}^{n-1} \frac{\binom{k+M-1}{k}}{2^{M+k}}\\
            &= \sum_{k=0}^{n} \frac{-n \binom{k+(M-1)-1}{k} + M \binom{k+M-2}{k-1}}{2^{(M-1)+k}}\\
            &= \sum_{k=0}^{n} \frac{-n \frac{(M+k-2) \cdots (M-1)}{k!} + M \frac{(M+k-2) \cdots M}{(k-1)!}}{2^{(M-1)+k}}\\
            &= \sum_{k=0}^{n} \frac{(M+k-2) \cdots M}{k!} \cdot \frac{-n(M-1) + M k}{2^{(M-1)+k}}\\
            &= \sum_{k=0}^{n} \frac{(M+k-2) \cdots M}{k!} \cdot \frac{n - (n-k)M}{2^{(M-1)+k}}\\
            &\approx \frac{(M+(n-1)-2) \cdots M}{(n-1)!} \cdot \frac{n - (n-(n-1))M}{2^{(M-1)+(n-1)}} + \frac{(M+n-2) \cdots M}{n!} \cdot \frac{n - (n-n)M}{2^{(M-1)+n}}\\
            &= \frac{(M+n-3) \cdots M}{(n-1)!} \cdot \frac{n - M}{2^{M+n-2}} + \frac{(M+n-2) \cdots M}{n!} \cdot \frac{n}{2^{M+n-1}}\\
            &= \frac{(M+n-3) \cdots M}{(n-1)!} \cdot \left(\frac{n - M}{2^{M+n-2}} + \frac{M+n-2}{2^{M+n-1}}\right)\\
            &= \frac{(M+n-3) \cdots M}{(n-1)! \cdot 2^{M+n-1}} \cdot \left(2(n - M) + (M+n-2)\right)\\
            &= \frac{(M+n-3) \cdots M}{(n-1)! \cdot 2^{M+n-1}} \cdot (3n - 2 - M)\\
            &\approx -\frac{M^{n-1}}{(n-1)! \cdot 2^{M+n-1}}
        \end{align*}
        \begin{align*}
            \EE(Y^2) - n(n+2)
            &= -n \sum_{k=0}^{n} \frac{\binom{k+(M-1)-1}{k}}{2^{(M-1)+k}} -n(n+1) \sum_{k=0}^{n+1} \frac{\binom{k+(M-2)-1}{k}}{2^{(M-2)+k}} + M^2 \cdot \sum_{k=0}^{n-1} \frac{\binom{k+M-1}{k}}{2^{M+k}}\\
            &= \sum_{k=0}^{n+1} \frac{-n \binom{k+(M-1)-2}{k-1} - n(n+1) \binom{k+(M-2)-1}{k} + M^2 \binom{k+M-3}{k-2}}{2^{(M-2)+k}}\\
            &= \sum_{k=0}^{n+1} \frac{(M+k-3) \cdots M}{k!}\\
            &\qquad \cdot \frac{-nk (M-1) - n(n+1) (M-1)(M-2) + M^2 (k-1)k}{2^{(M-2)+k}}\\
            &= \sum_{k=0}^{n+1} \frac{(M+k-3) \cdots M}{k!}\\
            &\qquad \cdot \frac{M^2 ((k-1)k - n(n+1)) + Mn(3(n+1) - k) + n(k - 2(n+1))}{2^{(M-2)+k}}
        \end{align*}
        \begin{align*}
            &\approx \frac{(M+n-3) \cdots M}{n!}\\
            &\qquad \cdot \frac{M^2 ((n-1)n - n(n+1)) + Mn(3(n+1) - n) + n(n - 2(n+1))}{2^{(M-2)+n}}\\
            &\qquad + \frac{(M+(n+1)-3) \cdots M}{(n+1)!}\\
            &\qquad \cdot \frac{M^2 (((n+1)-1)(n+1) - n(n+1)) + Mn(3(n+1) - (n+1)) + n((n+1) - 2(n+1))}{2^{(M-2)+(n+1)}}\\
            &= \frac{(M+n-3) \cdots M}{n!} \cdot \frac{M^2 (-2n) + Mn(2n+3) - n(n+2)}{2^{M+n-2}}\\
            &\qquad + \frac{(M+n-2) \cdots M}{(n+1)!} \cdot \frac{Mn(2(n+1)) - n(n+1)}{2^{M+n-1}}\\
            &= \frac{(M+n-3) \cdots M}{(n-1)!} \cdot \frac{M^2 (-2) + M(2n+3) - (n+2)}{2^{M+n-2}}\\
            &\qquad + \frac{(M+n-3) \cdots M}{(n-1)!} \cdot \frac{(M+n-2)(2M - 1)}{2^{M+n-1}}\\
            &= \frac{(M+n-3) \cdots M}{(n-1)!} \cdot \left(\frac{-2M^2 + (2n+3)M - (n+2)}{2^{M+n-2}} + \frac{2M^2 + (2n - 5)M - (n-2)}{2^{M+n-1}}\right)\\
            &= \frac{(M+n-3) \cdots M}{(n-1)! \cdot 2^{M+n-1}} \cdot \left(-4M^2 + (4n+6)M - (2n+4) + 2M^2 + (2n - 5)M - (n-2)\right)\\
            &= \frac{(M+n-3) \cdots M}{(n-1)! \cdot 2^{M+n-1}} \cdot \left(-2M^2 + (6n+1)M - (3n+2)\right)\\
            &\approx -\frac{M^{n}}{(n-1)! \cdot 2^{M+n-2}}
        \end{align*}
        Следовательно
        \begin{gather*}
            \EE(Y) = n - \frac{M^{n-1}}{(n-1)! \cdot 2^{M+n-1}} (1 + o(1))\\
            \EE(Y)^2 = n^2 - 2n\frac{M^{n-1}}{(n-1)! \cdot 2^{M+n-1}} (1 + o(1))\\
            \EE(Y^2) = n(n+2) - \frac{M^{n}}{(n-1)! \cdot 2^{M+n-2}} (1 + o(1))\\
            \DD(Y) = \EE(Y^2) - \EE(Y)^2 = 2n - \frac{M^{n}}{(n-1)! \cdot 2^{M+n-2}} (1 + o(1))\\
        \end{gather*}
    \end{enumproblem}

    \begin{enumproblem}
        WLOG будем считать, что у поездов скорости равны $1$, \dots, $n$ соответственно, но выезжают они в случайном порядке. Рассмотрим всякий элементарный исход. По сути он соответствует расстановке поездов по временам выезда. Давайте сопоставим каждому такому исходу перестановку чисел от $1$ до $n$, определённую следующим образом.
        \begin{figure}[h]
            \centering
            \Large
            \inkscapepicture{DPT-list-001-solutions-01}
        \end{figure}
        \begin{quotation}
            Рассмотрим любой караван. Пусть в строимой перестановке скорость первого поезда каравана перейдёт в скорость второго поезда каравана, скорость второго --- в скорость третьего, и т.д., скорость последнего --- в скорость первого поезда каравана.
        \end{quotation}
        Таким образом караваны переводятся в циклы этой перестановки. Заметим, что данное сопоставление биективно, так как объектов обоих типов по $n!$, а отображение сюръективное (так как циклы всякой перестановки можно записать по порядку элементов в этих циклах, начиная с самых малых элементов циклов, и расставить эти циклы в порядке возрастания минимальных элементов). Поэтому задача теперь сведена к поиску мат. ожидания и дисперсии количества циклов в случайной (вероятность равномерно распределена) перестановки.

        Теперь будем сопоставлять каждой перестановке последовательность чисел по следующему правилу.
        \begin{quotation}
            Будем по очереди удалять элементы перестановки в порядке от $n$ до $1$ и писать последовательность с конца. Пусть на некотором шаге мы удаляем вершину $k$. Если в её цикле нет других элементов, то напишем $0$ и удалим цикл; иначе напишем номер вершины, в которую $k$ переходит и удалим вершину $k$, сопоставляя вершине, которая переходила в $k$, вершину, в которую переходит $k$.
        \end{quotation}
        Например, перестановке с рисунка будет сопоставлена последовательность $(0; 0; 0; 3; 4; 4; 2)$. Причём, обращая операции (если написан ноль --- создать новый цикл, иначе --- вставить вершину в уже имеющийся в правильное место), можно по всякой последовательности получить перестановку. Единственное условие на последовательность --- она должна иметь вид $(a_1; \dots; a_n)$, где $a_k \in \{0; \dots; k-1\}$. При этом количество циклов в перестановке в таком случае перейдёт в количество нулей в последовательности. Поэтому теперь задача сведена к поиску мат. ожидания и дисперсии количества нулей в случайной (вероятность равномерно распределена) последовательности описанного вида.

        Заметим, что полученную случайную величину уже понятным образом можно превратить в сумму случайных независимых величин $X_k = \mathds{1}_{a_k = 0}$ ($k \in \{1; \dots; n\}$). Значит
        \[\EE(X) = \sum_{k=1}^n \EE(X_k) = \sum_{k=1}^n \frac{1}{k}\]
        и следовательно
        \[\DD(X) = \sum_{k=1}^n \DD(X_k) = \sum_{k=1}^n \EE(X_k^2) - \EE(X_k)^2 = \sum_{k=1}^n \EE(X_k) - \EE(X_k)^2 = \sum_{k=1}^n \frac{1}{k} - \frac{1}{k^2}\]
    \end{enumproblem}

    \begin{enumproblem}
        Нет. :(
    \end{enumproblem}

    \begin{enumproblem}
        Определим
        \begin{align*}
            &p := \PP(X \geqslant t)
            &a := \EE(X \mid X \geqslant t)&
            &b := \EE(X \mid X < t)
        \end{align*}
        Тогда понятно, что
        \begin{gather*}
            \EE(X) = p \cdot \EE(X \mid X \geqslant t) + (1-p) \cdot \EE(X \mid X < t) = pa + (1-p)b\\
            \DD(X) = p \cdot \EE((X - \EE(X))^2 \mid X \geqslant t) + (1-p) \cdot \EE((X - \EE(X))^2 \mid X < t)
        \end{gather*}
        Также повторим, что для всякой константы $\lambda$ и всякой случайной величины $T$
        \[
            \EE((T - \lambda)^2)
            = \EE(T^2) - 2\lambda \EE(T) + \lambda^2
            = (\lambda - \EE(T))^2 + \EE(T^2) - \EE(T)^2
            = (\lambda - \EE(T))^2 + \DD(T)
            \geqslant (\lambda - \EE(T))^2
        \]
        В частности применим это утверждение к величинам $X \mid X \geqslant t$ и $X \mid X < t$ и константе $\EE(X)$:
        \begin{gather*}
            \EE((X - \EE(X))^2 \mid X \geqslant t) \geqslant (\EE(X) - \EE(X \mid X \geqslant t))^2 = (0 - a)^2 = a^2\\
            \EE((X - \EE(X))^2 \mid X < t) \geqslant (\EE(X) - \EE(X \mid X < t))^2 = (0 - b)^2 = b^2
        \end{gather*}
        Поэтому
        \[
            \DD(X)
            \geqslant p a^2 + (1-p) b^2
            = p a^2 + \frac{((1-p)b)^2}{(1-p)}
            = p a^2 + \frac{(-pa)^2}{(1-p)}
            = \frac{pa^2}{(1-p)}
            \geqslant \frac{pt^2}{(1-p)}
        \]
        так как $a = \EE(X \mid X \geqslant t) \geqslant t$. Следовательно
        \begin{align*}
            \DD(X) (1-p) &\geqslant p t^2&
            \DD(X) &\geqslant p (t^2 + \DD(X))&
            \frac{\DD(X)}{\DD(X) + t^2} &\geqslant p = \PP\{X \geqslant t\}&
        \end{align*}
    \end{enumproblem}

    \begin{enumproblem}
        Заметим, что при всяком элементарном исходе если $S_n \leqslant c$, то $X_1 \leqslant c$, \dots, $X_n \leqslant c$. Следовательно
        \[\PP\{S_n \leqslant c\} \leqslant \PP\left\{\bigwedge_{k=1}^n X_k \leqslant c\right\}\]
        При этом из независимости и одинаковой распределённости случайных величин $X_1$, \dots, $X_n$ следует, что
        \[
            \PP\left\{\bigwedge_{k=1}^n X_k \leqslant c\right\}
            = \prod_{k=1}^n \PP\{X_k \leqslant c\}
            = (\PP\{X_1 \leqslant c\})^n
            = F(c)^n
        \]
        Таким образом
        \[\PP\{S_n \leqslant c\} \leqslant F(c)^n\]
        Поскольку $F(c) < 1$, то можно взять в качестве $\alpha$ значение $-\ln(F(c))$. Единственная проблема, когда так сделать нельзя --- когда $F(c) = 0$; но в таком случае подойдёт любая константа $\alpha > 0$.

        \begin{remark*}
            На самом деле среди всех оценок вида $e^{\alpha n + \beta}$ оценка $\alpha = -\ln(F(c))$, $\beta = 0$ является наилучшей. Действительно, если фиксировать $N \in \NN$ и рассмотреть величины
            \[
                X_k := \begin{cases}
                    c/N& \text{c вероятностью $p$}\\
                    c + 1& \text{c вероятностью $1-p$}\\
                \end{cases}
            \]
            где, понятное дело, $p = F(c)$, то для всех $n \leqslant N$
            \[\PP\{S_n \leqslant c\} = \PP\left\{\bigwedge_{k=1}^n X_k \leqslant c\right\}\]
            (очевидно, что либо некоторая величина примет значение $c+1 > c$, либо все они примут $c/N$, а значит сумма будет равна $cn/N \leqslant c$). Таким образом для $n \leqslant N$
            \[F(c)^n = p^n \leqslant e^{\alpha n + \beta}\]
            Рассматривая случаи разных $N$ (и соответствующих им величин) мы получаем, что это же утверждение для всех $n$. Логарифмируя его получаем, что для всех $n \in \NN \cup \{0\}$
            \[\ln(p) n \leqslant \alpha n + \beta\]
            Следовательно $\alpha \geqslant \ln(p)$ (следует из асимптотики), а $\beta \geqslant 0$ (следует из значения при $n = 0$).
        \end{remark*}
    \end{enumproblem}

    \begin{enumproblem}
        Пусть $\rho(n, k)$ есть вероятность попасть ровно $k + 1$ из $n + 2$ бросков (т.е. $k$ новых раз из $n$ новых бросков); $0 \leqslant k \leqslant n$. Покажем по индукции по $n$, что $\rho(n, k) = 1/(n+1)$.

        \textbf{База.} $n = 0$. Очевидно, вероятность равна $1 = 1/(n+1)$, так как новых бросков не было совершено.

        \textbf{Шаг.} Пусть утверждение доказано для $n$; докажем его для $n+1$. Как нам дано в условии, если мы забросили $k+1$ мяч из $n+2$, то с вероятностью $(k+1)/(n+2)$ мы забросим ещё один, а с вероятностью $(n-k+1)/(n+2)$ --- не забросим. Следовательно
        \begin{multline*}
            \rho(n+1, k)
            = \frac{k}{n+2}\rho(n, k-1) + \frac{n-k+1}{n+2}\rho(n, k)\\
            = \frac{k}{n+2}\cdot\frac{1}{n+1} + \frac{n-k+1}{n+2}\cdot\frac{1}{n+1}
            = \frac{n+1}{n+2}\cdot\frac{1}{n+1}
            = \frac{1}{n+2}
        \end{multline*}
        При этом в крайних случаях (т.е. при $k = 0$ или $k = n+1$) ничего не ломается: если $k = 0$, то мы просто получаем, что
        \[\rho(n+1, k) = \rho(n+1, 0) = \frac{n+1}{n+2} \rho(n, 0) = \frac{1}{n+1}\]
        Аналогично и для $k = n+1$.

        Следовательно требуемая в задаче величина $\rho(100, 50)$ просто равна $1/101$.
    \end{enumproblem}

    \begin{enumproblem}
        Рассмотрим возможные конфигурации ящика: $0$ белых и $100$ чёрных (обозначим как $(0; 100)$), $1$ белый и $99$ чёрных ($(1; 99)$), \dots, $100$ белых и $0$ чёрных ($(100; 0)$). Каждый раз, находясь в конфигурации $(w; b)$ мы берём случайную (вероятность равномерно распределена) упорядоченную пару шаров и перекрашиваем первый в цвет второго. Говоря иначе,
        \[
            \xymatrix{
                & {(w; b)} \ar[dl]_{wb} \ar[dr]^{wb} \ar[d] &\\
                {(w-1; b+1)} & {(w; b)} & {(w+1; b-1)}
            }
        \]
        с вероятностью $wb/((w+b)(w+b-1))$ мы берём пару из белого и чёрного шаров и переходим в $(w-1; b+1)$, с той же вероятностью берём пару из чёрного и белого шаров и переходим в $(w+1; b-1)$, а с оставшейся вероятностью остаёмся на месте. А нас спрашивают мат. ожидание числа шагов, после которых мы попадём в $(0; 100)$ или $(100; 0)$ (и там останемся, так как из них выйти нельзя), начиная в $(1; 99)$.

        Обозначим за $E(w)$ искомое мат. ожидания, если стартовать в $(w; N - w)$, нде $N$ --- количество шаров. Несложно видеть, что для всякого $k \in \{1; \dots; N-1\}$ выполнено следующее равенство:
        \[E(k) = 1 + \frac{k(N-k)}{N(N-1)} E(k-1) + \frac{k(N-k)}{N(N-1)} E(k+1) + \left(1-\frac{2k(N-k)}{N(N-1)}\right) E(k)\]
        Перепишем его немного в другом виде:
        \[2E(k) - E(k-1) - E(k+1) = \frac{N(N-1)}{k(N-k)}\]
        При этом понятно, что $E(0) = E(N) = 0$. Следовательно мы имеем уравнение
        \[
            \begin{pmatrix}
                1\\
                -1& 2& -1\\
                & \ddots& \ddots& \ddots\\
                && -1& 2& -1\\
                &&&& 1\\
            \end{pmatrix}
            \begin{pmatrix}
                E(0)\\ E(1)\\ \vdots\\ E(N-1)\\ E(N)
            \end{pmatrix}
            =
            \begin{pmatrix}
                0\\ \frac{N(N-1)}{1(N-1)}\\ \vdots\\ \frac{N(N-1)}{(N-1)1}\\ 0
            \end{pmatrix}
        \]
        
        Временно отойдём от данного уравнения и рассмотрим свойства некоторых матриц. Для начала обозначим за $A_n$ матрицу размера $n \times n$ вида
        \[
            \begin{pmatrix}
                2& -1\\
                -1& \ddots& \ddots\\
                & \ddots& \ddots& -1\\
                && -1& 2
            \end{pmatrix}
        \]
        Докажем по индукции по $n$, что $\det(A_n) = n+1$.

        \textbf{База.} $\det(A_0) = 1$, $\det(A_1) = 2$, $\det(A_2) = 2 \cdot 2 - (-1) \cdot (-1) = 3$.

        \textbf{Шаг.} Рассмотрим ненулевые слагаемые из определения $\det$ (как сумма по всем перестановкам). Если слагаемое использует $a_{1,1}$, то сумма всех таких слагаемых равна $2 \cdot \det(A_{n-1})$; иначе оно использует $a_{1, 2}$ и $a_{2, 1}$ (иначе занулится), а значит сумма таких слагаемых равна $- (-1)^2 \cdot \det(A_{n-2})$. Таким образом
        \[\det(A_n) = 2\det(A_{n-1}) - \det(A_{n-2}) = 2n - (n-1) = n+1\]

        Теперь давайте найдём $A_n^{-1}$; обозначим её за $B_n$. Вспомним, что
        \[b_{i, j} = \frac{(-1)^{i+j} M_{j, i}}{\det(A_n)}\]
        где $M_{i, j}$ --- дополнительные миноры. Пусть $i \leqslant j$; иначе будет верно аналогичное рассуждение. Тогда сам минор $M_{i, j}$ имеет вид
        \[
            \begin{pmatrix}[cccc|ccccc|cccc]
                2& -1&&&&&&&\\
                -1& \ddots& \ddots&&&&&&\\
                & \ddots& \ddots& -1&&&&&\\
                && -1& 2& -1&&&&\\
                \hline
                &&&& -1& 2& -1&&\\
                &&&&& \ddots& \ddots& \ddots&\\
                &&&&&& \ddots& \ddots& -1\\
                &&&&&&& \ddots& 2&\\
                &&&&&&&& -1& -1\\
                \hline
                &&&&&&&&& 2& -1\\
                &&&&&&&&& -1& \ddots& \ddots\\
                &&&&&&&&&& \ddots& \ddots& -1\\
                &&&&&&&&&&& -1& 2
            \end{pmatrix}
        \]
        где по вертикали отчёркнуты $i-1$, $j-i$ и $n-j$ столбцов соответственно. Заметим, что все ненулевые слагаемые определителя $M_{i, j}$ в первых $i-1$ столбцах используют элементы из верхнего левого сектора, а значит и первые $i-1$ строк; следовательно $-1$ в среднем верхнем секторе минора не используется. Аналогично не используется определитель в среднем правом секторе. Следовательно
        \[
            M_{i,j} =
            \det(A_{i-1})
            \cdot
            \underbrace{
                \begin{vmatrix}
                    -1& 2& -1\\
                    & \ddots& \ddots& \ddots\\
                    && \ddots& \ddots& -1\\
                    &&& \ddots& 2\\
                    &&&& -1\\
                \end{vmatrix}
            }_{j-i}
            \cdot
            \det(A_{n-j})
            = i \cdot (-1)^{j-i} \cdot (n+1 - j)
        \]
        В общем случае мы имеем
        \[M_{i, j} = (-1)^{i+j} \min(i, j) \cdot (n+1 - \max(i, j))\]
        а следовательно
        \[b_{i, j} = \frac{\min(i, j) \cdot (n+1 - \max(i, j))}{n+1}\]

        Теперь вернёмся назад. Заметим, что
        \begin{align*}
            \left(
                \underbrace{
                    \begin{matrix}
                        1\\
                        -1& 2& -1\\
                        & \ddots& \ddots& \ddots\\
                        && -1& 2& -1\\
                        &&&& 1\\
                    \end{matrix}
                }_{N+1}
            \right)^{-1}
            &=
            \left(
                \begin{pmatrix}
                    1\\
                    -1& 1&\\
                    && \ddots\\
                    &&& 1& -1\\
                    &&&& 1\\
                \end{pmatrix}
                \begin{pmatrix}
                    1\\
                    & 2& -1\\
                    & -1& \ddots& \ddots\\
                    && \ddots& \ddots& -1\\
                    &&& -1& 2&\\
                    &&&&& 1\\
                \end{pmatrix}
            \right)^{-1}\\
            &=
            \begin{pmatrix}
                1\\
                & A_{N-1}\\
                &&1
            \end{pmatrix}^{-1}
            \begin{pmatrix}
                1\\
                -1& 1\\
                && \ddots\\
                &&& 1& -1\\
                &&&& 1\\
            \end{pmatrix}^{-1}\\
            &=
            \begin{pmatrix}
                1\\
                & A_{N-1}^{-1}\\
                &&1
            \end{pmatrix}
            \begin{pmatrix}
                1\\
                1& 1\\
                && \ddots\\
                &&& 1& 1\\
                &&&& 1\\
            \end{pmatrix}\\
            &=
            \begin{pmatrix}
                1\\
                & B_{N-1}\\
                &&1
            \end{pmatrix}
            \begin{pmatrix}
                1\\
                1& 1\\
                && \ddots\\
                &&& 1& 1\\
                &&&& 1\\
            \end{pmatrix}
        \end{align*}
        Следовательно
        \[
            \begin{pmatrix}
                E(0)\\ E(1)\\ \vdots\\ E(N-1)\\ E(N)
            \end{pmatrix}
            =
            \begin{pmatrix}
                1\\
                & B_{N-1}\\
                &&1
            \end{pmatrix}
            \begin{pmatrix}
                1\\
                1& 1\\
                && \ddots\\
                &&& 1& 1\\
                &&&& 1\\
            \end{pmatrix}
            \begin{pmatrix}
                0\\ \frac{N(N-1)}{1(N-1)}\\ \vdots\\ \frac{N(N-1)}{(N-1)1}\\ 0
            \end{pmatrix}
        \]
        Весь вектор посчитать сложно из-за природы $B_{N-1}$, но нам ведь нужно посчитать только $E(1)$! Так что получим, что
        \begin{align*}
            E(1)
            &=
            \left(
                \begin{pmatrix}
                    1\\
                    & B_{N-1}\\
                    &&1
                \end{pmatrix}
                \begin{pmatrix}
                    1\\
                    1& 1\\
                    && \ddots\\
                    &&& 1& 1\\
                    &&&& 1\\
                \end{pmatrix}
            \right)_{2, *}
            \begin{pmatrix}
                0\\ \frac{N(N-1)}{1(N-1)}\\ \vdots\\ \frac{N(N-1)}{(N-1)1}\\ 0
            \end{pmatrix}\\
            &=
            \begin{pmatrix}
                1\\
                & B_{N-1}\\
                &&1
            \end{pmatrix}_{2, *}
            \begin{pmatrix}
                1\\
                1& 1\\
                && \ddots\\
                &&& 1& 1\\
                &&&& 1\\
            \end{pmatrix}
            \begin{pmatrix}
                0\\ \frac{N(N-1)}{1(N-1)}\\ \vdots\\ \frac{N(N-1)}{(N-1)1}\\ 0
            \end{pmatrix}\\
            &=
            \begin{pmatrix}
                0& (B_{N-1})_{1, *}& 0
            \end{pmatrix}
            \begin{pmatrix}
                0\\ \frac{N(N-1)}{1(N-1)}\\ \vdots\\ \frac{N(N-1)}{(N-1)1}\\ 0
            \end{pmatrix}\\
            &= \sum_{k=1}^{N-1} \frac{\min(1, k) \cdot (N - \max(1, k))}{N} \cdot \frac{N(N-1)}{k(N-k)}\\
            &= \sum_{k=1}^{N-1} \frac{1 \cdot (N - k)}{N} \cdot \frac{N(N-1)}{k(N-k)}\\
            &= (N-1)\sum_{k=1}^{N-1} \frac{1}{k}
        \end{align*}
        В случае $N = 100$ мы имеем
        \[
            99 \sum_{k=1}^{99} \frac{1}{k}
            = \frac{360\ 968\ 703\ 235\ 711\ 654\ 233\ 892\ 612\ 988\ 250\ 163\ 157\ 207}{704\ 246\ 214\ 441\ 540\ 173\ 379\ 129\ 383\ 184\ 972\ 763\ 200} \approx 512.56
        \]
    \end{enumproblem}
\end{document}